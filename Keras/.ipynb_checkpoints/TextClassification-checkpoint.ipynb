{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None,\n",
    "                                                        test_split=0.2)\n",
    "word_index = reuters.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of classes will be the maximum value in the y_train + 1,  \n",
    "since the classes start from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training sample: 8982\n",
      "# of training sample: 2246\n",
      "# of classes: 46\n"
     ]
    }
   ],
   "source": [
    "print('# of training sample: {}'.format(len(x_train)))\n",
    "print('# of training sample: {}'.format(len(x_test)))\n",
    "\n",
    "num_classes = max(y_train) + 1\n",
    "print('# of classes: {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1, 27595, 28842, 8 ..]  \n",
    "what this means is that, the sentence is composed of, 1st most frequent word, 27595 most frequent word, basically each of these numbers is mapped to a word and the numebr is how frequent that word is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'money' is the 236 most frequent word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['money']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {}\n",
    "for k, v in word_index.items():\n",
    "    index_to_word[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'money'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[236]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the wattie nondiscriminatory mln loss for plc said at only ended said commonwealth could 1 traders now april 0 a after said from 1985 and from foreign 000 april 0 prices its account year a but in this mln home an states earlier and rise and revs vs 000 its 16 vs 000 a but 3 psbr oils several and shareholders and dividend vs 000 its all 4 vs 000 1 mln agreed largely april 0 are 2 states will billion total and against 000 pct dlrs\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([index_to_word[x] for x in x_train[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 10000)\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "(8982, 46)\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train[0])\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words, )))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/10\n",
      "8083/8083 [==============================] - 2s 229us/step - loss: 0.1128 - acc: 0.9598 - val_loss: 1.0343 - val_acc: 0.8042\n",
      "Epoch 2/10\n",
      "8083/8083 [==============================] - 2s 229us/step - loss: 0.1225 - acc: 0.9600 - val_loss: 1.0421 - val_acc: 0.7998\n",
      "Epoch 3/10\n",
      "8083/8083 [==============================] - 2s 226us/step - loss: 0.1247 - acc: 0.9608 - val_loss: 1.0599 - val_acc: 0.8031\n",
      "Epoch 4/10\n",
      "8083/8083 [==============================] - 2s 231us/step - loss: 0.1183 - acc: 0.9629 - val_loss: 1.0631 - val_acc: 0.8031\n",
      "Epoch 5/10\n",
      "8083/8083 [==============================] - 2s 261us/step - loss: 0.1203 - acc: 0.9607 - val_loss: 1.0793 - val_acc: 0.7998\n",
      "Epoch 6/10\n",
      "8083/8083 [==============================] - 2s 235us/step - loss: 0.1165 - acc: 0.9620 - val_loss: 1.0321 - val_acc: 0.8042\n",
      "Epoch 7/10\n",
      "8083/8083 [==============================] - 2s 243us/step - loss: 0.1158 - acc: 0.9640 - val_loss: 1.0720 - val_acc: 0.7976\n",
      "Epoch 8/10\n",
      "8083/8083 [==============================] - 2s 244us/step - loss: 0.1076 - acc: 0.9641 - val_loss: 1.0924 - val_acc: 0.8031\n",
      "Epoch 9/10\n",
      "8083/8083 [==============================] - 2s 235us/step - loss: 0.1180 - acc: 0.9618 - val_loss: 1.0640 - val_acc: 0.7998\n",
      "Epoch 10/10\n",
      "8083/8083 [==============================] - 2s 235us/step - loss: 0.1068 - acc: 0.9625 - val_loss: 1.1181 - val_acc: 0.7898\n",
      "2246/2246 [==============================] - 0s 125us/step\n",
      "Test loss: 1.0624586941403995\n",
      "Test accuray: 0.7996438118037855\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs\n",
    "                   , verbose=1, validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.0624586941403995\n",
      "Test accuray: 0.7996438118037855\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0}'.format(score[0], 2))\n",
    "print('Test accuray: {0}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
